from keras.layers import Input, Lambda
from keras.layers.core import Dense, Flatten, RepeatVector, Dropout
from keras.layers.convolutional import Convolution1D
from keras.layers.recurrent import GRU
from keras.layers.normalization import BatchNormalization
from keras.models import load_model
from keras import backend as K
from keras.models import Model
from keras.layers.merge import Concatenate
from tgru_k2_gpu import TerminalGRU


# =============================
# Encoder functions
# =============================
def encoder_model(params):
    # K_params is dictionary of keras variables
    x_in = Input(shape=(params["max_len"], params["nchars"]), name="input_molecule_smi")
    # shape(120, 35)

    # Convolution layers
    if params["encoder_type"] == "CNN":
        x = Convolution1D(int(20), 6, activation="tanh", name="encoder_conv0")(x_in)
        x = BatchNormalization(axis=-1, name="encoder_norm0")(x)

        x = Convolution1D(int(20), 6, activation="tanh", name="encoder_conv1")(x)
        x = BatchNormalization(axis=-1, name="encoder_norm1")(x)

    elif params["encoder_type"] == "RNN":
        x = GRU(int(20), return_sequences=True, activation="tanh", name="encoder_gru0")(x_in)
        # x = BatchNormalization(axis=-1, name="encoder_norm0")(x)
        x = GRU(int(20), return_sequences=True, activation="tanh", name="encoder_gru1")(x)
        # x = BatchNormalization(axis=-1, name="encoder_norm1")(x)

    x = Flatten()(x)

    # Middle layers
    # middle = Dense(128, activation="tanh", name="encoder_dense0")(x)
    # middle = Dropout(params["dropout_rate"])(middle)
    # middle = BatchNormalization(axis=-1, name="encoder_dense0_norm")(middle)

    middle = Dense(params["hidden_dim"], activation="tanh", name="encoder_dense1")(x)
    middle = Dropout(params["dropout_rate"])(middle)
    middle = BatchNormalization(axis=-1, name="encoder_dense1_norm")(middle)

    z_mean = Dense(params["hidden_dim"], name="z_mean_sample")(middle)
    # return both mean and last encoding layer for std dev sampling
    return Model(x_in, [z_mean, middle], name="encoder")


def load_encoder(params):
    # Need to handle K_params somehow...
    # Also are we going to be able to save this layer?
    # encoder = encoder_model(params, K.constant(0))
    # encoder.load_weights(params["encoder_weights_file"])
    # return encoder
    # !# not sure if this is the right format
    return load_model(params["encoder_weights_file"])


# ===========================================
# Decoder functions
# ===========================================


def decoder_model(params):
    z_in = Input(shape=(params["hidden_dim"],), name="decoder_input")
    true_seq_in = Input(shape=(params["max_len"], params["nchars"]),
                        name="decoder_true_seq_input")

    z = Dense(int(params["hidden_dim"]), activation="tanh", name="decoder_dense0")(z_in)
    z = Dropout(params["dropout_rate"])(z)
    z = BatchNormalization(axis=-1, name="decoder_dense0_norm")(z)


    # Necessary for using GRU vectors
    z_reps = RepeatVector(params["max_len"])(z)

    # Encoder parts using GRUs
    x_dec = GRU(int(128), return_sequences=True, activation="tanh",name="decoder_gru0")(z_reps)
    x_dec = GRU(int(64), return_sequences=True, activation="tanh", name="decoder_gru1")(x_dec)

    x_out = TerminalGRU(params["nchars"], rnd_seed=params["rand_seed"], recurrent_dropout=params["dropout_rate"], return_sequences=True, #dropput=0.2
                        activation="softmax", temperature=0.01, name="decoder_tgru", implementation=0)([x_dec, true_seq_in])

    return Model([z_in, true_seq_in], x_out, name="decoder")


def load_decoder(params):
    return load_model(params["decoder_weights_file"], custom_objects={"TerminalGRU": TerminalGRU})


##====================
## Middle part (var)
##====================

def variational_layers(z_mean, enc, kl_loss_var, params):
    # @inp mean : mean generated from encoder
    # @inp enc : output generated by encoding
    # @inp params : parameter dictionary passed throughout entire model.

    def sampling(args):
        z_mean, z_log_var = args
        epsilon = K.random_normal_variable(shape=(params["batch_size"], params["hidden_dim"]), mean=0., scale=1., seed=params["rand_seed"])
        z_rand = z_mean + K.exp(z_log_var / 2) * kl_loss_var * epsilon
        return K.in_train_phase(z_rand, z_mean)

    z_log_var_layer = Dense(params["hidden_dim"], name="z_log_var_sample")
    z_log_var = z_log_var_layer(enc)
    z_mean_log_var_output = Concatenate(name="z_mean_log_var")([z_mean, z_log_var])
    z_samp = Lambda(sampling)([z_mean, z_log_var])
    return z_samp, z_mean_log_var_output


# ====================
# Property Prediction
# ====================

def property_predictor_model(params):
    ls_in = Input(shape=(params["hidden_dim"],), name="prop_pred_input")

    prop_mid = Dense(int(64), activation="sigmoid")(ls_in)
    prop_mid = Dropout(params["dropout_rate"])(prop_mid)

    prop_mid = Dense(int(32), activation="softplus", name="property_predictor_dense0")(prop_mid)
    prop_mid = Dropout(params["dropout_rate"])(prop_mid)

    reg_prop_pred = Dense(len(params["reg_prop_tasks"]), activation="linear", name="reg_property_output")(prop_mid)
    return Model(ls_in, reg_prop_pred, name="property_predictor")

def load_property_predictor(params, modify = False):
    if modify:
        return load_model("../models/zinc_properties/zinc_prop_pred.h5")
    else:
        return load_model(params["prop_pred_weights_file"])
